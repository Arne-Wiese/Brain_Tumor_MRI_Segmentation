{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4cdfcf",
   "metadata": {},
   "source": [
    "# 2.5D Convolutional Network for Brain Tumor Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57e802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural imaging\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84882bbe",
   "metadata": {},
   "source": [
    "## Download the brats dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77ef0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/arnewiese/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"awsaf49/brats20-dataset-training-validation\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = path + '/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ba35b",
   "metadata": {},
   "source": [
    "## Create the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18230f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for BraTS 2020 brain tumor segmentation.\n",
    "\n",
    "    Loads 3D MRI volumes and extracts 2.5D slices for training by aggregating\n",
    "    multiple consecutive slices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patient_list, data_dir, slice_range=(2, 153), n_slices=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patient_list: List of patient folder names (e.g., ['BraTS20_Training_001', ...])\n",
    "            data_dir: Path to the data directory\n",
    "            slice_range: Tuple (min_slice, max_slice) to focus on brain region with tumors\n",
    "            n_slices: Number of consecutive slices to aggregate (must be odd for symmetry)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.slice_range = slice_range\n",
    "        self.n_slices = n_slices\n",
    "        self.half_slices = n_slices // 2\n",
    "        self.samples = []  # Will store (patient_id, slice_index) pairs\n",
    "\n",
    "        # For each patient, identify which slices contain tumor\n",
    "        for patient_id in patient_list:\n",
    "            # Only include slices within our range that have enough context slices\n",
    "            for slice_idx in range(slice_range[0], slice_range[1]):\n",
    "                self.samples.append((patient_id, slice_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of samples (2D slices)\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return one sample (a single 2D slice with aggregated context from n slices).\n",
    "\n",
    "        Returns:\n",
    "            image: Tensor of shape (4, 240, 240) - 4 MRI modalities with aggregated slices\n",
    "            mask: Tensor of shape (240, 240) - segmentation labels (center slice)\n",
    "        \"\"\"\n",
    "        patient_id, slice_idx = self.samples[idx]\n",
    "        patient_path = os.path.join(self.data_dir, patient_id)\n",
    "\n",
    "        # Load all 4 modalities for this slice and neighboring slices\n",
    "        modalities = ['flair', 't1', 't1ce', 't2']\n",
    "        aggregated_slices = []\n",
    "\n",
    "        for modality in modalities:\n",
    "            file_path = os.path.join(patient_path, f\"{patient_id}_{modality}.nii\")\n",
    "            volume = nib.load(file_path).get_fdata()\n",
    "            \n",
    "            # Collect n_slices around the center slice\n",
    "            slices_to_aggregate = []\n",
    "            for offset in range(-self.half_slices, self.half_slices + 1):\n",
    "                idx_to_load = slice_idx + offset\n",
    "                slice_2d = volume[:, :, idx_to_load]\n",
    "                slice_2d = self.normalize(slice_2d)\n",
    "                slices_to_aggregate.append(slice_2d)\n",
    "            \n",
    "            # Aggregate by taking the mean across slices\n",
    "            aggregated_slice = np.mean(slices_to_aggregate, axis=0)\n",
    "            aggregated_slices.append(aggregated_slice)\n",
    "\n",
    "        # Stack into (4, 240, 240) tensor\n",
    "        image = np.stack(aggregated_slices, axis=0)\n",
    "\n",
    "        # Load segmentation mask (only the center slice)\n",
    "        seg_path = os.path.join(patient_path, f\"{patient_id}_seg.nii\")\n",
    "        seg_volume = nib.load(seg_path).get_fdata()\n",
    "        mask = seg_volume[:, :, slice_idx]\n",
    "\n",
    "        # Convert labels: 0->0, 1->1, 2->2, 4->3 (to have consecutive labels)\n",
    "        mask[mask == 4] = 3\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.FloatTensor(image)\n",
    "        mask = torch.LongTensor(mask.astype(np.int64))\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def normalize(self, slice_2d):\n",
    "        \"\"\"Normalize a 2D slice to [0, 1] range\"\"\"\n",
    "        min_val = slice_2d.min()\n",
    "        max_val = slice_2d.max()\n",
    "\n",
    "        if max_val - min_val > 0:\n",
    "            return (slice_2d - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            return slice_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f08eff",
   "metadata": {},
   "source": [
    "## Create Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abc7f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 258 patients\n",
      "Val: 55 patients\n",
      "Test: 56 patients\n"
     ]
    }
   ],
   "source": [
    "all_patients = sorted([d for d in os.listdir(TRAIN_DATASET_PATH)\n",
    "                      if os.path.isdir(os.path.join(TRAIN_DATASET_PATH, d))])\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "train_patients, temp_patients = train_test_split(\n",
    "    all_patients, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: split temp into 50/50 (15% val, 15% test of original)\n",
    "val_patients, test_patients = train_test_split(\n",
    "    temp_patients, test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_patients)} patients\")\n",
    "print(f\"Val: {len(val_patients)} patients\")\n",
    "print(f\"Test: {len(test_patients)} patients\")\n",
    "\n",
    "\n",
    "# Create datasets from preprocessed arrays\n",
    "train_dataset = BraTSDataset(train_patients,TRAIN_DATASET_PATH)\n",
    "val_dataset = BraTSDataset(val_patients, TRAIN_DATASET_PATH)\n",
    "test_dataset = BraTSDataset(test_patients, TRAIN_DATASET_PATH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09bf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e725767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic U-Net architecture for brain tumor segmentation.\n",
    "    \n",
    "    Input: (batch_size, 4, 240, 240) - 4 MRI modalities\n",
    "    Output: (batch_size, 4, 240, 240) - 4 segmentation classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels=4, n_classes=4):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ffd4b",
   "metadata": {},
   "source": [
    "## Initialize Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44fa1dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model has 31044292 parameters\n"
     ]
    }
   ],
   "source": [
    "# Set device - prioritize MPS for Mac, then CUDA, then CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = UNet(n_channels=4, n_classes=4).to(device)\n",
    "\n",
    "# Loss function - CrossEntropyLoss for multi-class segmentation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d3923",
   "metadata": {},
   "source": [
    "## Dice Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bcbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(pred, target, num_classes=4):\n",
    "    \"\"\"\n",
    "    Calculate Dice score for multi-class segmentation.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted logits (B, C, H, W)\n",
    "        target: Ground truth labels (B, H, W)\n",
    "        num_classes: Number of segmentation classes\n",
    "    \n",
    "    Returns:\n",
    "        Mean Dice score across all classes\n",
    "    \"\"\"\n",
    "    pred = torch.argmax(pred, dim=1)  # (B, H, W)\n",
    "    dice_scores = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = (pred == class_id).float()\n",
    "        target_class = (target == class_id).float()\n",
    "        \n",
    "        intersection = (pred_class * target_class).sum()\n",
    "        union = pred_class.sum() + target_class.sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            dice = 1.0  # Perfect score if both are empty\n",
    "        else:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        \n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29a2b6",
   "metadata": {},
   "source": [
    "## Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "411cb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_score(outputs, masks)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return epoch_loss, epoch_dice\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice_score(outputs, masks)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return epoch_loss, epoch_dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cbb9f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223f2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_loss, train_dice = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dice\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     14\u001b[39m outputs = model(images)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     18\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tumor_seg/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tumor_seg/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tumor_seg/lib/python3.14/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tumor_seg/lib/python3.14/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_val_dice = 0.0\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_dices = []\n",
    "val_losses = []\n",
    "val_dices = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_dice = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_dice = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_dices.append(train_dice)\n",
    "    val_losses.append(val_loss)\n",
    "    val_dices.append(val_dice)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_dice': val_dice,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"âœ“ Saved new best model with Dice: {val_dice:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation Dice score: {best_val_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30348d3",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b953d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Dice score\n",
    "axes[1].plot(train_dices, label='Train Dice', marker='o')\n",
    "axes[1].plot(val_dices, label='Val Dice', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice Score')\n",
    "axes[1].set_title('Training and Validation Dice Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99b9ec",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee72858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataloader, device, num_samples=3):\n",
    "    \"\"\"Visualize some predictions from the model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, masks = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Move to CPU for visualization\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Show FLAIR modality\n",
    "        axes[i, 0].imshow(images[i, 0], cmap='gray')\n",
    "        axes[i, 0].set_title('FLAIR')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Show T1 modality\n",
    "        axes[i, 1].imshow(images[i, 1], cmap='gray')\n",
    "        axes[i, 1].set_title('T1')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Show T1CE modality\n",
    "        axes[i, 2].imshow(images[i, 2], cmap='gray')\n",
    "        axes[i, 2].set_title('T1CE')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Show ground truth mask\n",
    "        axes[i, 3].imshow(masks[i], cmap='tab10', vmin=0, vmax=3)\n",
    "        axes[i, 3].set_title('Ground Truth')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        # Show prediction\n",
    "        axes[i, 4].imshow(predictions[i], cmap='tab10', vmin=0, vmax=3)\n",
    "        axes[i, 4].set_title('Prediction')\n",
    "        axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some validation samples\n",
    "visualize_predictions(model, val_loader, device, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ee994",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af09063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with val Dice: {checkpoint['val_dice']:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_dice = validate(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Dice Score: {test_dice:.4f}\")\n",
    "\n",
    "# Visualize test predictions\n",
    "visualize_predictions(model, test_loader, device, num_samples=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
